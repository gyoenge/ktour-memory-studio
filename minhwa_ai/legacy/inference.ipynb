{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe33e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:11<00:00,  1.83s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# 학습 결과물이 저장된 경로\n",
    "model_path = \"minhwa_textual_inversion\"\n",
    "\n",
    "# 스테이블 디퓨전 파이프라인 불러오기\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 학습된 '민화 스타일' 임베딩 불러오기\n",
    "pipeline.load_textual_inversion(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d09f0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 placeholder_token (<minhwa-style>)을 사용하여 이미지 생성\n",
    "prompt = \"A korean tiger in the style of <minhwa-style>\"\n",
    "image = pipeline(prompt, num_inference_steps=50).images[0]\n",
    "\n",
    "# 이미지 저장\n",
    "image.save(\"minhwa_style_tiger.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8435dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트에 placeholder_token (<minhwa-style>)을 사용하여 이미지 생성\n",
    "prompt = \"A korean tiger\"\n",
    "image = pipeline(prompt, num_inference_steps=50).images[0]\n",
    "\n",
    "# 이미지 저장\n",
    "image.save(\"no_style_tiger.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a08ef9",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67d6b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:01,  2.72it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:03<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# 스테이블 디퓨전 파이프라인 불러오기\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트에 placeholder_token (<minhwa-style>)을 사용하여 이미지 생성\n",
    "prompt = \"A korean tiger in the style of <minhwa-style>\"\n",
    "image = pipeline(prompt, num_inference_steps=50).images[0]\n",
    "\n",
    "# 이미지 저장\n",
    "image.save(\"minhwa_style_tiger_notrain.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964006d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트에 placeholder_token (<minhwa-style>)을 사용하여 이미지 생성\n",
    "prompt = \"A korean tiger\"\n",
    "image = pipeline(prompt, num_inference_steps=50).images[0]\n",
    "\n",
    "# 이미지 저장\n",
    "image.save(\"no_style_tiger_notrain.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466faea4",
   "metadata": {},
   "source": [
    "## lora inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f154d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:14<00:00,  2.10s/it]\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# 1. LoRA 모델 가중치가 저장된 경로 (학습 시 --output_dir 경로와 동일)\n",
    "lora_model_path = \"minhwa_lora_model/checkpoint-1500\"\n",
    "\n",
    "# 2. 베이스 모델 경로 (매우 중요! 학습 시 --pretrained_model_name_or_path 경로와 동일)\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# 3. 스테이블 디퓨전 파이프라인 불러오기 (반드시 학습에 사용한 베이스 모델로!)\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 4. LoRA 가중치 불러오기 및 파이프라인에 적용\n",
    "# .bin 또는 .safetensors 형식의 LoRA 가중치 파일을 자동으로 찾아서 로드합니다.\n",
    "pipeline.load_lora_weights(lora_model_path)\n",
    "\n",
    "# 5. GPU로 모델 이동\n",
    "pipeline.to(\"cuda\")\n",
    "\n",
    "# 6. 프롬프트 작성 (LoRA 방식)\n",
    "# <special-token> 없이, 학습 데이터의 특징을 설명하는 단어들을 조합합니다.\n",
    "# 학습용 metadata.jsonl에 넣었던 \"a minhwa painting\", \"sansudo\", \"tiger\" 같은 키워드를 활용하면 효과적입니다.\n",
    "prompt = \"a minhwa painting of a tiger in a landscape, sansudo style\"\n",
    "negative_prompt = \"blurry, worst quality, low quality\" # 필요 시 부정적 프롬프트 추가\n",
    "\n",
    "# 7. 이미지 생성\n",
    "# guidance_scale은 프롬프트를 얼마나 따를지 정하는 값으로, 7~8 사이가 안정적입니다.\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5\n",
    ").images[0]\n",
    "\n",
    "# 8. 이미지 저장\n",
    "image.save(\"minhwa_lora_tiger_checkpoint-1500.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c853a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:00<00:02, 19.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"tiger\"(호랑이)를 \"rabbit\"(토끼)으로 변경\n",
    "prompt = \"a minhwa painting of a rabbit\"\n",
    "negative_prompt = \"blurry, worst quality, low quality\" # 필요 시 부정적 프롬프트 추가\n",
    "\n",
    "# 7. 이미지 생성\n",
    "# guidance_scale은 프롬프트를 얼마나 따를지 정하는 값으로, 7~8 사이가 안정적입니다.\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5\n",
    ").images[0]\n",
    "\n",
    "# 8. 이미지 저장 - *** 저장 파일 이름을 변경합니다 ***\n",
    "image.save(\"minhwa_lora_rabbit_checkpoint-1500.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e7b9c",
   "metadata": {},
   "source": [
    "### lora with category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:14<00:00,  2.08s/it]\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      " 52%|█████▏    | 26/50 [00:01<00:01, 21.30it/s]"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# 1. LoRA 모델 가중치가 저장된 경로 (학습 시 --output_dir 경로와 동일)\n",
    "lora_model_path = \"minhwa_lora_model_withcategory_r32\"\n",
    "\n",
    "# 2. 베이스 모델 경로 (매우 중요! 학습 시 --pretrained_model_name_or_path 경로와 동일)\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# 3. 스테이블 디퓨전 파이프라인 불러오기 (반드시 학습에 사용한 베이스 모델로!)\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 4. LoRA 가중치 불러오기 및 파이프라인에 적용\n",
    "# .bin 또는 .safetensors 형식의 LoRA 가중치 파일을 자동으로 찾아서 로드합니다.\n",
    "pipeline.load_lora_weights(lora_model_path)\n",
    "\n",
    "# 5. GPU로 모델 이동\n",
    "pipeline.to(\"cuda\")\n",
    "\n",
    "# 6. 프롬프트 작성 (LoRA 방식)\n",
    "# <special-token> 없이, 학습 데이터의 특징을 설명하는 단어들을 조합합니다.\n",
    "# 학습용 metadata.jsonl에 넣었던 \"a minhwa painting\", \"sansudo\", \"tiger\" 같은 키워드를 활용하면 효과적입니다.\n",
    "# prompt = \"a minhwa painting of a tiger in a landscape, sansudo style\"\n",
    "prompt = \"a minhwa painting, HODO, Tiger, in a landscape.\"\n",
    "negative_prompt = \"blurry, worst quality, low quality\" # 필요 시 부정적 프롬프트 추가\n",
    "\n",
    "# 7. 이미지 생성\n",
    "# guidance_scale은 프롬프트를 얼마나 따를지 정하는 값으로, 7~8 사이가 안정적입니다.\n",
    "image = pipeline(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5\n",
    ").images[0]\n",
    "\n",
    "# 8. 이미지 저장\n",
    "image.save(\"minhwa_lora_tiger_withcategory_r32_prompting.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74a020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:02<00:05,  1.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m base_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 3. 스테이블 디퓨전 파이프라인 불러오기 (반드시 학습에 사용한 베이스 모델로!)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 4. LoRA 가중치 불러오기 및 파이프라인에 적용\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# .bin 또는 .safetensors 형식의 LoRA 가중치 파일을 자동으로 찾아서 로드합니다.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mload_lora_weights(lora_model_path)\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:1025\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m     sub_model_dtype \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1021\u001b[0m         torch_dtype\u001b[38;5;241m.\u001b[39mget(name, torch_dtype\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch_dtype\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[0;32m-> 1025\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_model_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1051\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1052\u001b[0m     )\n\u001b[1;32m   1054\u001b[0m init_kwargs[name] \u001b[38;5;241m=\u001b[39m loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_loading_utils.py:849\u001b[0m, in \u001b[0;36mload_sub_model\u001b[0;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors, dduf_entries, provider_options, quantization_config)\u001b[0m\n\u001b[1;32m    847\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name)):\n\u001b[0;32m--> 849\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloading_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[1;32m    852\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(cached_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:288\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:5176\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5167\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5169\u001b[0m     (\n\u001b[1;32m   5170\u001b[0m         model,\n\u001b[1;32m   5171\u001b[0m         missing_keys,\n\u001b[1;32m   5172\u001b[0m         unexpected_keys,\n\u001b[1;32m   5173\u001b[0m         mismatched_keys,\n\u001b[1;32m   5174\u001b[0m         offload_index,\n\u001b[1;32m   5175\u001b[0m         error_msgs,\n\u001b[0;32m-> 5176\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5182\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5185\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5193\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5194\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:5639\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5636\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5638\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m-> 5639\u001b[0m         _error_msgs, disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5640\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[1;32m   5642\u001b[0m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:946\u001b[0m, in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m--> 946\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:815\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    813\u001b[0m param \u001b[38;5;241m=\u001b[39m param[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[1;32m    817\u001b[0m     param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# --- 설정값 ---\n",
    "\n",
    "# 1. 체크포인트들이 저장된 기본 폴더 경로\n",
    "# 학습 명령어의 --output_dir 경로와 동일해야 합니다.\n",
    "base_output_dir = \"minhwa_lora_model_withcategory_r32\" \n",
    "\n",
    "# 2. 베이스 모델 경로 (학습 시 사용한 모델과 동일)\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# 3. 생성할 프롬프트 정보\n",
    "prompt = \"a minhwa painting of a tiger in a landscape, sansudo style\"\n",
    "negative_prompt = \"blurry, worst quality, low quality\"\n",
    "\n",
    "# 4. 반복할 에폭 및 스텝 정보\n",
    "num_epochs = 15\n",
    "steps_per_epoch = 162\n",
    "\n",
    "# --- 설정 끝 ---\n",
    "\n",
    "\n",
    "# 5. 스테이블 디퓨전 파이프라인을 한 번만 불러옵니다. (효율성)\n",
    "print(\"Loading base pipeline...\")\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.to(\"cuda\")\n",
    "print(\"Pipeline loaded.\")\n",
    "\n",
    "# 6. 각 에폭별 체크포인트에 대해 루프를 실행합니다.\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # 현재 에폭에 해당하는 글로벌 스텝 계산\n",
    "    global_step = epoch * steps_per_epoch\n",
    "    \n",
    "    # 로드할 LoRA 가중치 경로 구성\n",
    "    lora_model_path = os.path.join(base_output_dir, f\"checkpoint-{global_step}\")\n",
    "    \n",
    "    # 생성될 이미지 파일 이름 구성\n",
    "    output_image_name = f\"minhwa_lora_tiger_withcategory_r32_prompting_{epoch}.png\"\n",
    "    \n",
    "    print(f\"\\n--- Processing Epoch {epoch}/{num_epochs} (Checkpoint: {global_step}) ---\")\n",
    "\n",
    "    # 체크포인트 폴더가 실제로 존재하는지 확인\n",
    "    if not os.path.exists(lora_model_path):\n",
    "        print(f\"Warning: Checkpoint directory not found, skipping: {lora_model_path}\")\n",
    "        continue # 다음 루프로 넘어감\n",
    "\n",
    "    try:\n",
    "        # 7. LoRA 가중치 불러오기 및 파이프라인에 적용\n",
    "        # load_lora_weights는 내부적으로 이전 LoRA 가중치를 해제하고 새로 로드합니다.\n",
    "        pipeline.load_lora_weights(lora_model_path)\n",
    "        print(f\"Successfully loaded LoRA weights from {lora_model_path}\")\n",
    "\n",
    "        # 8. 이미지 생성\n",
    "        image = pipeline(\n",
    "            prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=50,\n",
    "            guidance_scale=7.5\n",
    "        ).images[0]\n",
    "        \n",
    "        # 9. 이미지 저장\n",
    "        image.save(output_image_name)\n",
    "        print(f\"Image saved successfully: {output_image_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {lora_model_path}: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nAll epochs processed. Image generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d646f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
